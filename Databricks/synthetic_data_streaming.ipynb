{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Live Data Streaming for Hotel Bookings\n",
    "\n",
    "*Note: This notebook can be run either in Databricks or with a spark environment running locally*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process:\n",
    "\n",
    "- In this notebook, we are simulating the live data streaming for hotel bookings using spark and confluent cloud (kafka). The streaming data is setup to generate data for a new hotel booking every 15 mins and writes the data to a topic in confluent cloud (kafka)\n",
    "\n",
    "### Use Case:\n",
    "\n",
    "- The stream will be read using a databricks notebook every 7 days and using the endpoint created for predictions, we will calculate the probability of cancellations for every active booking.\n",
    "\n",
    "- Email campaigns will be triggered using a pipeline for bookings which have a probability of cancellation higher than 70% (configurable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "### 1.1 Verify Installation of Confluent's Python client for Apache Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Other Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1 Import pyspark modules for stream processing\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import Trigger\n",
    "\n",
    "# 1.2.2 Import os for masking the environment variables (api key and secret for confluent cloud)\n",
    "import os\n",
    "\n",
    "# 1.2.3 Import time to control the stream data processing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the step-by-step process of simulating, processing, and publishing streaming data to a Kafka topic. This workflow is designed to represent live hotel bookings made by customers.\n",
    "\n",
    "### 2.1 Define a Schema for Stream Processing\n",
    "\n",
    "Initially, we create a schema corresponding to the Avro schema specified for the Kafka topic. This schema organizes the streaming data and includes fields such as:\n",
    "\n",
    "- `hotel`\n",
    "- `lead_time`\n",
    "- `arrival_date_month`\n",
    "- `arrival_date_week_number`\n",
    "- `arrival_date_day_of_month`\n",
    "- `stays_in_weekend_nights`\n",
    "- `stays_in_week_nights`\n",
    "- `adults`\n",
    "- `children`\n",
    "- `babies`\n",
    "- `meal`\n",
    "- `country`\n",
    "- `market_segment`\n",
    "- `distribution_channel`\n",
    "- `is_repeated_guest`\n",
    "- `previous_cancellations`\n",
    "- `previous_bookings_not_canceled`\n",
    "- `reserved_room_type`\n",
    "- `booking_changes`\n",
    "- `deposit_type`\n",
    "- `days_in_waiting_list`\n",
    "- `customer_type`\n",
    "- `adr`\n",
    "- `required_car_parking_spaces`\n",
    "- `total_of_special_requests`\n",
    "\n",
    "These fields are tailored to capture the dynamics of hotel bookings, from guest details to reservation specifics.\n",
    "\n",
    "\n",
    "### 2.2 Simulate Streaming Data\n",
    "\n",
    "To mimic the flow of real-time data, a streaming DataFrame is created that generates one row every 15 minutes. This is made with the assumption that the hotel receives a new booking every 15 minutes.\n",
    "\n",
    "### 2.3 Transform Streaming Data\n",
    "\n",
    "The generated streaming data is then transformed to align with the previously defined schema. This step involves casting and structuring data fields to ensure they match the expected format for subsequent processing and analysis.\n",
    "\n",
    "### 2.4 Publish to Kafka\n",
    "\n",
    "With the streaming data properly formatted, the next step is to publish this data to a specific Kafka topic. This involves configuring Kafka connection parameters, and sending the transformed data to the designated Kafka topic, making it available for real-time consumption.\n",
    "\n",
    "### 2.5 Stream Execution and Termination\n",
    "\n",
    "The streaming process is executed for a predetermined duration (e.g., 30 minutes) to simulate a live data feed. After this period, the streaming query is programmatically terminated, stopping the data flow and concluding the simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Schema columns based on the provided hotel booking schema\n",
    "schema_columns = [\"hotel\", \"lead_time\", \"arrival_date_month\", \"arrival_date_week_number\",\n",
    "                  \"arrival_date_day_of_month\", \"stays_in_weekend_nights\", \"stays_in_week_nights\",\n",
    "                  \"adults\", \"children\", \"babies\", \"meal\", \"country\", \"market_segment\",\n",
    "                  \"distribution_channel\", \"is_repeated_guest\", \"previous_cancellations\",\n",
    "                  \"previous_bookings_not_canceled\", \"reserved_room_type\", \"booking_changes\",\n",
    "                  \"deposit_type\", \"days_in_waiting_list\", \"customer_type\", \"adr\",\n",
    "                  \"required_car_parking_spaces\", \"total_of_special_requests\"]\n",
    "\n",
    "# 2.2 Create a streaming DataFrame that generates one row per second\n",
    "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load().trigger(Trigger.ProcessingTime(\"15 minutes\"))\n",
    "\n",
    "# 2.3 Transform this DataFrame to match our hotel booking schema\n",
    "df_stream_transformed = df_stream.selectExpr(\"CAST(value AS STRING) AS key\",\n",
    "    \"\"\"to_json(struct(\n",
    "        'City Hotel' as hotel,\n",
    "        cast(value as int) % 300 as lead_time,\n",
    "        'January' as arrival_date_month,\n",
    "        cast((value % 53) + 1 as int) as arrival_date_week_number,\n",
    "        cast((value % 31) + 1 as int) as arrival_date_day_of_month,\n",
    "        cast((value % 5) as int) as stays_in_weekend_nights,\n",
    "        cast((value % 10) as int) as stays_in_week_nights,\n",
    "        cast((value % 4) + 1 as int) as adults,\n",
    "        cast((value % 3) as int) as children,\n",
    "        cast((value % 2) as int) as babies,\n",
    "        'BB' as meal,\n",
    "        'USA' as country,\n",
    "        'Online TA' as market_segment,\n",
    "        'TA/TO' as distribution_channel,\n",
    "        cast(value % 2 as int) as is_repeated_guest,\n",
    "        0 as previous_cancellations,\n",
    "        0 as previous_bookings_not_canceled,\n",
    "        'A' as reserved_room_type,\n",
    "        cast((value % 5) as int) as booking_changes,\n",
    "        'No Deposit' as deposit_type,\n",
    "        0 as days_in_waiting_list,\n",
    "        'Transient' as customer_type,\n",
    "        cast(100 + (rand() * 120) as float) as adr,\n",
    "        0 as required_car_parking_spaces,\n",
    "        cast((value % 3) as int) as total_of_special_requests\n",
    "    )) AS value\"\"\")\n",
    "\n",
    "# 2.4 Write the stream data to the kafka topic\n",
    "kafka_api_key = os.getenv('KAFKA_API_KEY', 'default_api_key')\n",
    "kafka_api_secret = os.getenv('KAFKA_API_SECRET', 'default_api_secret')\n",
    "\n",
    "topic_name = \"hotel_booking_live\"\n",
    "bootstrap_server = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "\n",
    "ds = df_stream_transformed \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_server)\\\n",
    "    .option(\"subscribe\", topic_name)\\\n",
    "    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n",
    "    .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_api_key}\" password=\"{kafka_api_secret}\";')\\\n",
    "  .option(\"topic\", topic_name) \\\n",
    "  .option(\"checkpointLocation\", \"/dbfs/dir\") \\\n",
    "  .trigger(processingTime='10 seconds') \\\n",
    "  .start()\n",
    "\n",
    "\n",
    "# 2.5 Let the stream run for 30 minutes and terminate\n",
    "time.sleep(1800)\n",
    "ds.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
