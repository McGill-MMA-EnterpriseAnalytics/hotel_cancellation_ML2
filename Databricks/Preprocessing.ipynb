{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2adc8c-075c-4201-a122-c70c8e4a5a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, upper, sum as spark_sum, count, lit\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define file paths\n",
    "read_data_path = \"/mnt/blobstorage1\"\n",
    "\n",
    "# Read data into Spark DataFrames from mounted ADLS location\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(read_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6179bec-3c90-4db2-a628-1c5489ad533a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the 'reservation_status_date' and 'reservation_status_days_difference' columns\n",
    "df = df.drop('reservation_status_date', 'reservation_status', 'assigned_room_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8a3cc3-c487-4bd6-bbbf-2f1ac15218c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the 'agent' and 'company' columns\n",
    "df = df.drop('agent', 'company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04368bfc-ce57-4183-b66a-b06e5789b324",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming a Spark session is already created and named spark\n",
    "# If not, you need to create one as follows:\n",
    "# spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# 1. Sorting the DataFrame\n",
    "df = df.orderBy(['name', 'arrival_date_year', 'arrival_date_month', 'arrival_date_day_of_month'])\n",
    "\n",
    "# 2. Creating a 'num_bookings' column\n",
    "window_spec = Window.partitionBy('name').orderBy('arrival_date_year', 'arrival_date_month', 'arrival_date_day_of_month')\n",
    "df = df.withColumn('num_bookings', F.row_number().over(window_spec) - 1)\n",
    "\n",
    "# 3. Dropping the 'arrival_date_year' column\n",
    "df = df.drop('arrival_date_year')\n",
    "\n",
    "# 4. Conditionally replacing country names\n",
    "# First, compute the country counts and create a broadcast variable (for efficiency in larger datasets)\n",
    "country_counts = df.groupBy('country').count()\n",
    "broadcast_country_counts = spark.sparkContext.broadcast({row['country']: row['count'] for row in country_counts.collect()})\n",
    "df = df.withColumn('country', F.when(df['country'].isin([k for k, v in broadcast_country_counts.value.items() if v > 1000]), df['country']).otherwise(F.lit('Other')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614bec16-4e6f-4500-b10c-dc10d2e33a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('name', 'email','phone-number', 'credit_card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c846352c-df92-4a76-80d8-63344aa3f349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to a single CSV file in a directory\n",
    "output_path_dir = \"/mnt/blobstorage1\"\n",
    "df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(output_path_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
